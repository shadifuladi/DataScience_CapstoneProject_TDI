{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "import pandas as pd\n",
    "\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill \n",
    "\n",
    "df = dill.load(open('df_preprocessed.pkd', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Sentiment Analysis  \n",
    "\n",
    "This notebook aims at processing the text messages and using natural language processing techniques to evaluate the sentiment around each text message. \n",
    "\n",
    "In particular, this notebook uses [Stanford CoreNLP API](https://stanfordnlp.github.io/CoreNLP/index.html) sentiment tool. The Sentiment Annotator of CoreNLP implements [Socher et al](https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)'s sentiment model, by attaching a binary tree in the sentence level. The Node of the tree contains predicted class and scores for that subtree. The current version of the sentiment annotator of CoreNLP includes 5 score classes: very negative, negative, neutral, positive, and very positive. \n",
    "\n",
    "\n",
    "To record the sentiment scores of text messages, this notebook extracts the probability distribution associated with the 5 score classes (very negative to very positive) for each sentence within a message. A score of -2, -1, 0, 1, 2 is assigned to the 5 classes, and with that, the expected score for each sentence is calculated as: \n",
    "\n",
    "$$ E_i = \\sum_{j=1}^5 P_{i,j}(s) \\times s_{i,j} $$ \n",
    "\n",
    "where, $E_{i}$ is expected score for sentence *i*, and $P_j$ is the probability associated with each score class, $s_j$ in (-2,-1,0,1,2). \n",
    "\n",
    "Ultimately, the frequency of score classes for the sentences included in one text message is calculated by binning the expected scores of all sentences. \n",
    "\n",
    "The average of expected score for each message is claculated to indicate the overall score of a whole text message. \n",
    "\n",
    "Throughout the sentiment analysis the number of sentences and words within each message is extracted and added to the data frame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SentimentAnalysis(text):\n",
    "    \n",
    "    ''' runs the sentiment analysis using Stanford CoreNLP API,\n",
    "    returns:\n",
    "    (i) average sentiment over all sentences in a message, \n",
    "    (ii) the number of sentences and words,\n",
    "    (iii) and distribution of sentiments of all sentences in a message.'''\n",
    "\n",
    "    sent = nlp.annotate(text,\n",
    "                   properties={\n",
    "                       'annotators': 'sentiment, ner, pos',\n",
    "                       'outputFormat': 'json',\n",
    "                       'timeout': 150000})\n",
    "    \n",
    "    num_sentence = len(sent['sentences'])\n",
    "    num_words = len(text.split())\n",
    "    \n",
    "    sent_dist = [] \n",
    "    ''' sentiment distribution for each sentence\n",
    "     sentiment distribution is probability \n",
    "     for each sentiment score (-2 to 2).\n",
    "     it is obrtianed from the node \n",
    "     of the binary tree associated with each sentence. '''\n",
    "    scores = [-2,-1,0,1,2]\n",
    "    expected = []\n",
    "    sigma = []\n",
    "    tot_sent = 0\n",
    "    for i, s in enumerate(sent['sentences']):\n",
    "        sent_dist.append(s['sentimentDistribution'])\n",
    "        E_i = np.sum([a*b for a,b in zip(s['sentimentDistribution'],scores)])\n",
    "        s_i = np.sum([np.sqrt(a*b) for a,b in zip(s['sentimentDistribution']\n",
    "                                                  , [(x - E_i)**2 for x in scores])])\n",
    "        sigma.append(s_i)\n",
    "        expected.append(E_i)\n",
    "\n",
    "    mean_dist = np.mean(sent_dist,axis=0)\n",
    "    mean_expected_dist = np.sum(expected,axis=0)\n",
    "    freq = np.histogram(expected, np.linspace(-2,2,6))\n",
    "    \n",
    "    expected_text = np.sum([a*b for a,b in zip(mean_dist, scores)])\n",
    "    sigma_text = np.sum([np.sqrt(a*b) for a,b in zip(mean_dist, [(x - expected_text)**2 for x in scores])])\n",
    "\n",
    "    return num_sentence, num_words, expected_text, sigma_text, freq, sent_dist\n",
    "\n",
    "\n",
    "\n",
    "def get_sentiments(X):\n",
    "    \n",
    "    '''Modifies the original data frame \n",
    "    and adds the sentiment analysis outputs '''\n",
    "\n",
    "    cols = ['num_sentence',\n",
    "            'num_words',\n",
    "            'expected_sentiment_text',\n",
    "            'sigma_sentiment_text',\n",
    "            'freq-very_negative',\n",
    "            'freq-negative',\n",
    "            'freq-neutral',\n",
    "            'freq-positive',\n",
    "            'freq-very_positive']\n",
    "            #'sentiment_dist']\n",
    "\n",
    "    for i, c in enumerate(cols):\n",
    "        X[c] = 0\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        try:\n",
    "            text = str(X['clean_message'][i])\n",
    "            num_sentence, num_words, expected_text, sigma_text, freq, sentiment_dist = SentimentAnalysis(text)\n",
    "            X.loc[i,'num_sentence'] = num_sentence\n",
    "            X.loc[i,'num_words'] = num_words\n",
    "            X.loc[i,'expected_sentiment_text'] = expected_text\n",
    "            X.loc[i,'sigma_sentiment_text'] = sigma_text\n",
    "            X.loc[i,'freq-very_negative'] = freq[0][0]\n",
    "            X.loc[i,'freq-negative'] = freq[0][1]\n",
    "            X.loc[i,'freq-neutral'] = freq[0][2]\n",
    "            X.loc[i,'freq-positive'] = freq[0][3]\n",
    "            X.loc[i,'freq-very_positive'] = freq[0][4]\n",
    "            #X.loc[i,'sentiment_dist'] = sentiment_dist\n",
    "        except:\n",
    "            print(\"error where i =\", i) \n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentiment(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, func):\n",
    "        self.func = func\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None): \n",
    "        return self.func(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = Sentiment(get_sentiments)\n",
    "sent.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.dump(df, open('df_sentiments.pkd', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
